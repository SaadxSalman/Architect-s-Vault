# Persona-Agent ðŸ‘¤ðŸ¤–

A next-generation conversational agent that goes beyond text and audio to create a more natural and human-like interaction. Persona-Agent features a digital avatar with realistic facial expressions and body language, using real-time visual and auditory cues to make conversations feel more empathetic and engaging.

-----

## âœ¨ Features

  * **Natural Human-like Interaction:** The agent uses realistic facial expressions and body language to enhance the conversational experience.
  * **Real-time User Analysis:** The **Vision Agent** analyzes the user's emotional state, head movements, and eye gaze to provide real-time, context-aware responses.
  * **Multi-Modal Data Fusion:** Leverages **VideoMAE-v2** and **AudioCLIP** to seamlessly fuse visual and audio data, creating a comprehensive understanding of the user.
  * **Dynamic Synthesis:** The **Synthesis Agent** animates the digital human's avatar in real-time based on the agent's internal state and the conversation's flow.
  * **Core Conversational Intelligence:** The conversational logic is powered by a large-scale multimodal model, ensuring rich and contextually relevant dialogue.

-----

### ðŸ§  The Multimodal AI Stack (Inference Layer)

* **Inference Framework:** **FastAPI (Python)**. Chosen for its high speed and native support for asynchronous tasks, making it ideal for serving machine learning models.
* **Vision Analysis:** **VideoMAE-v2**. A masked autoencoder for video that excels at "Temporal Action Classification"â€”this allows the agent to recognize movements and facial changes over time rather than just static images.
* **Auditory Analysis:** **AudioCLIP**. A specialized model that maps audio signals to the same space as images and text, allowing the agent to "hear" the emotion in your voice.
* **Deep Learning Backend:** **PyTorch**. The engine that runs the actual model weights on the GPU.
* **Image Processing:** **OpenCV**. Used for real-time frame resizing, cropping, and normalization before feeding data into the AI.

---

### ðŸŽ¨ The Digital Human (Synthesis Layer)

* **3D Rendering:** **Three.js** via **@react-three/fiber**. This is the core of the **Synthesis Agent**, rendering the 3D character in the browser using WebGL.
* **Helper Library:** **@react-three/drei**. Provides essential utilities for loading `.glb` (3D) models and managing camera controls easily.
* **Animation Technique:** **Morph Targets (Blend Shapes)**. Instead of traditional "bones," the avatar uses vertex-based morphing to create fluid, realistic facial expressions like smiling, blinking, and talking.

---

### ðŸ“¡ Communication & Data Management

* **API Protocol:** **tRPC**. This is the "glue" of the project. It allows the Frontend to call functions on the Backend as if they were local, with full TypeScript autocompletion.
* **Validation:** **Zod**. Used to define schemas for the data moving between the frontend and backend, ensuring that every video frame and emotion tag is formatted correctly.
* **Database:** **MongoDB**. A NoSQL database used to store interaction logs, user preferences, and "memories" of previous conversations.
* **ODM:** **Mongoose**. Provides a structured way to interact with MongoDB within the Node.js environment.

---

### ðŸš€ DevOps & Infrastructure

* **Containerization:** **Docker**. Every part of the stack (the DB, the Node server, and the Python AI) is containerized to ensure it runs exactly the same on your machine as it would in production.
* **Orchestration:** **Docker Compose**. Allows you to spin up the entire multi-language ecosystem (Node + Python + Mongo) with a single command.
* **Styling:** **Tailwind CSS**. Used to build the "Command Center" UI overlay, handling the picture-in-picture webcam feed and chat interface.

---

### ðŸ“‚ Final Development Path

With this stack, your development flow for **saadsalmanakram/Persona-Agent** looks like this:

1. **Capture** user data in the browser (Next.js).
2. **Transmit** it via a type-safe tunnel (tRPC).
3. **Analyze** the behavior using the GPU (Python/PyTorch).
4. **Synthesize** the response by animating the avatar (Three.js).

-----

## ðŸš€ Getting Started

### Prerequisites

  * Node.js (for Next.js)
  * Access to a massive multimodal model and other APIs

### Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/saadsalmanakram/Persona-Agent.git
    cd Persona-Agent
    ```
2.  **Set up the front-end:**
    ```bash
    npm install
    ```
3.  **Configure the backend:**
    Follow the instructions in the `backend/` directory to set up the agent services and connect them to the necessary models.

### Configuration

Create a `.env` file to store your API keys and model credentials.

### Usage

Run the Next.js server and the backend services to start the agent. You can then interact with it via the web interface, which will capture your visual and auditory data for analysis.

-----

Here is the comprehensive file structure for **Persona-Agent**. This structure follows a **Monorepo** pattern, which is the industry standard for projects requiring tight integration between a Next.js frontend, a Node.js backend, and specialized AI microservices.

### ðŸ“‚ Project Structure

```text
Persona-Agent/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ web/                        # FRONTEND (Next.js)
â”‚   â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚   â”‚       â””â”€â”€ avatar.glb      # 3D Avatar assets
â”‚   â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx      # Root layout & Providers
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ page.tsx        # Main UI (Tailwind Layout)
â”‚   â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Avatar/
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ DigitalHuman.tsx # Three.js Synthesis Agent
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ PersonaClient.tsx    # Webcam & tRPC Logic
â”‚   â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ useWebcam.ts    # Stream management hook
â”‚   â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚   â”‚       â””â”€â”€ trpc.ts         # tRPC Client setup
â”‚   â”‚   â”œâ”€â”€ next.config.js
â”‚   â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â”‚   â””â”€â”€ package.json
â”‚   â”‚
â”‚   â””â”€â”€ server/                     # BACKEND (Node/Express)
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ models/
â”‚       â”‚   â”‚   â””â”€â”€ Interaction.ts  # MongoDB Schema
â”‚       â”‚   â”œâ”€â”€ services/
â”‚       â”‚   â”‚   â””â”€â”€ ai.service.ts   # Axios calls to FastAPI
â”‚       â”‚   â”œâ”€â”€ trpc/
â”‚       â”‚   â”‚   â”œâ”€â”€ router.ts       # Main API Router
â”‚       â”‚   â”‚   â””â”€â”€ context.ts      # Auth/DB context
â”‚       â”‚   â””â”€â”€ index.ts            # Server Entry Point
â”‚       â”œâ”€â”€ .env                    # DB and API Keys
â”‚       â”œâ”€â”€ tsconfig.json
â”‚       â””â”€â”€ package.json
â”‚
â”œâ”€â”€ services/
â”‚   â””â”€â”€ inference/                  # AI ENGINE (Python)
â”‚       â”œâ”€â”€ models/                 # Weight files (VideoMAE-v2, etc.)
â”‚       â”œâ”€â”€ main.py                 # FastAPI Server
â”‚       â”œâ”€â”€ processors/
â”‚       â”‚   â”œâ”€â”€ video.py            # VideoMAE logic
â”‚       â”‚   â””â”€â”€ audio.py            # AudioCLIP logic
â”‚       â”œâ”€â”€ Dockerfile              # PyTorch/CUDA environment
â”‚       â””â”€â”€ requirements.txt        # Python dependencies
â”‚
â”œâ”€â”€ docker-compose.yml              # Orchestrates Mongo + AI Engine
â”œâ”€â”€ package.json                    # Root workspace config
â””â”€â”€ README.md

```

---